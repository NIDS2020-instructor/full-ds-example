{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Basic Data Science Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with:\n",
    "* Terminal/Linux\n",
    "* Git\n",
    "* Python / Conda / Jupyter notebook / Visual Studio (VS) Code\n",
    "* Machine learning as implemented in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your aim is create a python package which can predict a country happiness score according to a set of features (details later on). For this project you will:\n",
    "* explore the data from the World Happiness Report 2020\n",
    "* train a machine learning (ML) model on these data\n",
    "* progressively move functions from your Jupyter notebook to a python module in VS code\n",
    "* use software version control with git as you implement your changes\n",
    "* include tests in your code so that to do continuous integration\n",
    "* create a python package called `happypred` for other users to easily use your code\n",
    "* show how to use your python package with a Jupyter notebook example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: setting up developping environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a conda environment `happy38` with the required python packages for our project, and start a new Jupyter notebook to explore the World Happiness data and start code prototyping. To type `conda` commands and create a new Jupyter server we will make use of two Jupyter terminal emulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Browser) Open a tab with the lecture Jupyter notebook server (`<YOUR IP>:8888`)\n",
    "2. (Browser) Open this Jupyter notebook (by clicking on `full_ds_example` in the file list)\n",
    "3. (Browser) Open two terminal tabs: on the Jupyter notebook server page, on the top right click on `New` and then `Terminal` (*do that twice!*)\n",
    "4. (Terminal 1) In your home directory create a directory `happiness_project` for your whole project\n",
    "5. (Terminal 1) Create a new conda environment `happy38` based on python 3.8 which will include the packages you need:\n",
    "  * `pandas`\n",
    "  * `xlrd` (to read Excel files)\n",
    "  * `seaborn`\n",
    "  * `scikit-learn`\n",
    "  * `ipykernel` (for jupyter notebook to see your new conda environment)\n",
    "  * `pylint` (for VS Code to interpret your code and tell you about errors before you run your code)\n",
    "  * `pytest` (to test your code)\n",
    "6. (Terminal 1) Launch a new Jupyter server dedicated to your project (type `jupyter notebook`)\n",
    "7. (Browser) Display your Jupyter server page by connecting to  `<YOUR IP>:8889` (please note this is a new server at port **8889** not the default lecture Jupyter server at port **8888**)\n",
    "8. (Browser) Start a new Jupyter notebook in the `happy38` environment to explore the data and prototype code (on the top right of your project Jupyter server page, click on `New` and then `Python (...) happy38`)\n",
    "9. (Browser) Rename this new notebook `happiness_project` by clicking on the title on the top left\n",
    "10. Make sure to have at least 6 tabs:\n",
    "  * One tab for the lecture Jupyter server (`<YOUR IP>:8888`)\n",
    "  * One tab for this notebook (`<YOUR IP>:8888/notebooks/full_ds_example.ipynb`)\n",
    "  * One tab with a busy terminal which created your project Jupyter server (`<YOUR IP>:8888/terminals/1`)\n",
    "  * One tab with an idle terminal (`<YOUR IP>:8888/terminals/2`) \n",
    "  * One tab with your project Jupyter notebook server (`<YOUR IP>:8889`)\n",
    "  * One tab with the notebook you just created (`<YOUR IP>:8889/notebooks/happiness_project.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Bash: \n",
    "    * `mkdir <DIRNAME>`: create directory `<DIRNAME>`\n",
    "    * `cd <DIRNAME>`: change to (i.e. enter inside) directory `<DIRNAME>`\n",
    "    * `ls`, or `ls <DIRNAME>`: list the content of the current dir, or list the content of dir `<DIRNAME>`\n",
    "* Conda\n",
    "    * `conda create --name <ENV_NAME> python=<X.Y>`: create conda environment named `<ENV_NAME>` based on python version `<X.Y>` (press `y` to proceed when prompted)\n",
    "    * `conda activate <ENV_NAME>`: activate conda environment `<ENV_NAME>`\n",
    "    * `conda install <PKG_NAME>`: install (python) package `<PKG_NAME>` in currently activated environment\n",
    "    * `conda install <PKG1_NAME> <PKG2_NAME> ...`: same as above to install several packages at the same time\n",
    "    * `conda deactivate`: deactivate current environment to go back to default `base` environment\n",
    "* Jupyter notebook\n",
    "    * `jupyter notebook` (to be run in conda `base` environment): start a Jupyter notebook having for root directory the current working directory\n",
    "    * To stop the notebook: on the terminal used to start the jupyter notebook, press `Ctrl+C` twice (or `Ctrl+C` once and then `y` to confirm shutting down the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploring the World Happiness Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the `pandas` and `seaborn` package to do basic data exploration of the World Happiness data, which measured happiness for different countries from 2006 to 2019. \n",
    "\n",
    "The aim is to predict happiness defined as a \"Life Ladder\": survey respondents had to place the status of their lives on a \"ladder\" scale ranging from 0 to 10, where 0 means the worst possible life and 10 the best possible life. \n",
    "\n",
    "In later parts, we will predict the Life Ladder (our label data `y`) from the following features (`X` data):\n",
    "* \"Log GDP per capita\"\n",
    "* \"Social support\" (national average of the binary responses (either 0 or 1) to question \"If you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?\"\n",
    "* \"Healthy life expectancy at birth\"\n",
    "* \"Freedom to make life choices\" (national average of binary responses to the question \"Are you satisfied or dissatisfied with your freedom to choose what you do with your life?\")\n",
    "* \"Generosity\" (calculated from responses to the question \"Have you donated money to a charity in the past month?\")\n",
    "* \"Perceptions of corruption\": the average of binary answers to two questions: \"Is corruption widespread throughout the government or not?\" and \"Is corruption widespread within businesses or not?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. (Terminal) Copy the Excel file `WHR20_DataForTable2.1.xls` from the directory `happiness_data` to your project directory `happiness_project`\n",
    "2. (Jupyter Explore) Use the `read_excel` function of the `pandas` package to read the data into a dataframe `wh_all_df`(no special arguments required aside the path to the Excel file)\n",
    "    * Tip: instead of a full path, the filename is enough since the Excel file is in the same directory as of your notebook\n",
    "3. (Jupyter Explore) Have a look at the dataframe by simply typing its name in a cell (`wh_all_df`)\n",
    "4. (Jupyter Explore) Create a dataframe `wh_withna_df` with only the columns of interest: \"year\", \"Life Ladder\", \"Log GDP per capita\", etc. (to simplify we will not consider \"Country name\" although in practice this could be quite important to account for repeated measures)\n",
    "    1. Extract all the columns of the dataframe using the `.columns` attribute\n",
    "    2. Use list slicing to only select the columns of interest from the previous list and save it in a variable called `cols_of_interest`\n",
    "    3. Create a new dataframe called `wh_withna_df` with by selecting only the columns `cols_of_interest` of `wf_all_df`\n",
    "5. (Jupyter Explore) Plot all the variable pairwise scatter plots with the `pairplot` function of the `seaborn` package\n",
    "6. (Jupyter Explore) Count the number of missing values (so called NA values) of each column\n",
    "7. (Jupyter Explore) Replace missing values, or remove rows with missing values, so that to obtain a dataframe `wh_df` without missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Bash\n",
    "    * `cp <FILE> <DIR>`: copy file `<FILE>` in directory `<DIR>`\n",
    "* Python `pandas` package\n",
    "    * ```\n",
    "    import pandas as pd\n",
    "    pd.<method>(<arg1>, <arg2>, ...)```: call the pandas method `<method>` with arguments `<arg1>`, `<arg2`>, etc.\n",
    "    * `new_df = old_df[colnames]`: create a new dataframe `new_df` from `old_df` by selecting only the `old_df` columns in the list `colnames`\n",
    "    * `df.isna()` return a dataframe where each entry of `df` is replaced by `True` if the entry was a missing value (e.g. `np.nan`) and with `False` if it was not missing\n",
    "    * `df.sum()` return for each column the total sum of its values (for boolean values, `True` counts as `1` and `False` counts as `0`)\n",
    "    * `df[colname].mean()` returns the mean of the column `colname` of the dataframe `df`\n",
    "    * `df[colname].median()` returns the median of the column `colname` of the dataframe `df`\n",
    "    * `df[colname].fillna(val)` returns a `Series` with all missing values in the column `colname` replaced with `val` (use `df[colname] = df[colname].fillna(val)` to apply to the column)\n",
    "    * `df.dropna()` returns a dataframe with all rows with missing values removed (use `new_df = df.dropna()` to save the result in the variable `new_df`)\n",
    "* Python `seaborn` package\n",
    "    * ```\n",
    "    import seaborn as sns\n",
    "    sns.pairplot(df)```: plot of the pairwise scatter plots between the columns of the dataframe `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you didn't finish\n",
    "Just enter the following commands:\n",
    "```\n",
    "cd\n",
    "cp -R happiness_solutions/part2/* happiness_project/\n",
    "```\n",
    "You are now up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Organizing the data and code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for the year 2019 will be left out and saved as `X_test.csv` and `y_test.csv`. It will be used to demonstrate the use of our package once finalized. The model will be trained and evaluated on the data from 2006 to 2018, which will be saved as `X_train.csv` to include the features (without the year) and `y_train.csv` which include the label variable (\"Life ladder\").\n",
    "\n",
    "We will write a python function in VS Code to undertake all the steps done so far and save the data in the different files (`X_train.csv`, `y_train.csv`, `X_test.csv`, `y_test.csv`). This function will be incorporated into a proper python package called `happypred` which will be imported in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Terminal 2) Go inside the `happiness_project` directory, and create a new `happypred` directory for the python package\n",
    "2. (Browser VS Code) Open a tab with VS Code (`<YOUR IP>:8080`) if not done already\n",
    "3. (VS Code) Open your `happypred` folder (from the top left menu, do `File` --> `Open Folder` and choose the `happypred` folder in the `happiness_project` directory)\n",
    "4. (VS Code) On the `Welcome` tab in the middle of the page, click on `New File` and save it as the module `happyio.py` in `happypred` (`io` is to remind us that this module is for input/output)\n",
    "5. (VS Code) In the module `happyio.py` create a function `whdata_to_csv`, which:\n",
    "    * Takes as input the path to an Excel file containing the World Happiness data\n",
    "    * Extracts the parent directory of that path and record it in a variable named `data_dir`\n",
    "    * Read all the data in a data_frame `wh_all_df`\n",
    "    * Deal with all missing values\n",
    "    * Extract the training data:\n",
    "        * Create a dataframe `X_train_df` with years up to 2018, and only the columns of interest (\"Log GDP per capita\", ..., \"Perceptions of corruption\"):\n",
    "        * Create a dataframe `y_train_df` with years up to 2018, and only the column \"Life Ladder\"\n",
    "    * Extract the testing data:\n",
    "        * Create a dataframe `X_test_df` with only year 2019, and only the columns of interest (\"Log GDP per capita\", ..., \"Perceptions of corruption\"):\n",
    "        * Create a dataframe `y_test_df` with only year 2019, and only the column \"Life Ladder\"\n",
    "    * Save the four dataframes `X_train_df`, `y_train_df`, `X_test_df`, `y_test_df` to the data directory `data_dir` as `X_train.csv`, `y_train.csv`, `X_test.csv`, `y_test.csv` respectively\n",
    "    * The function should return the paths to these four CSV files in that order\n",
    "6. (VS Code) Create a new file (`File` --> `New File`) and save it as `__init__.py` in the `happypred` directory\n",
    "7. (VS Code) In `__init__.py` make available for import the function `whdata_to_csv`\n",
    "8. (Jupyter Explore) Try to import the function `whdata_to_csv` from the `happypred` package you just defined\n",
    "    * Tip: add the following lines in your notebook to make sure your imported functions are updated if you modify your module\n",
    "    ```\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    ```\n",
    "9. (Jupyter Explore) Use the function to make sure it works\n",
    "10. (Terminal 2) Check all the CSV files were create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Bash: \n",
    "    * `mkdir <DIRNAME>`: create directory `<DIRNAME>`\n",
    "    * `ls -l`: list with the \"long listing format\" which displays file creation date and time \n",
    "* Python `os` package:\n",
    "    * `parentdir = os.path.dirname(my_file_path)`: to get the parent directory of a file with path `my_file_path` \n",
    "    * `file_path = os.path.join(my_dir, my_file)`: to create the path of a file `my_file` in a directory `my_dir`\n",
    "* Python `pandas` package:\n",
    "    * `row_condition = df[colX] == ...` (or `df[colX] <= ...`, etc.): to create a condition selecting rows from a dataframe `df` according to their values relative to a column `colX`\n",
    "    * `new_df = df.loc[row_condition, my_columns]`: to select only the columns `my_columns` and the rows satisfying the condition `row_condition` from a dataframe `df`  \n",
    "    * `new_df = df.drop(['my_col'], axis=1)`: create a new dataframe `new_df` after dropping the column `my_col`\n",
    "    * `df.to_csv(file_path, index=False)`: to save a dataframe `df` to a CSV file `file_path` (not saving the index which is not useful in our case)\n",
    "* Python `__init__` function to make available for import a function `my_function` in module `my_module`:\n",
    "\n",
    "`from .mymodule import my_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-76-fe0841ea356a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-76-fe0841ea356a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def whdata_to_csv(...):\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Python function skeleton\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def whdata_to_csv(...):\n",
    "    \"\"\" Read the World Happiness data at the provided path and extract training\n",
    "        data (years 2006 to 2018) and testing data (year 2019 only). \n",
    "        \n",
    "        For each training and testing dataset the label y is the \"Life Ladder\", and\n",
    "        the features X include \"Log GDP per capita\", \"Social support\", \n",
    "        \"Healthy life expectancy at birth\", \"Freedom to make life choices\", \"Generosity\"\n",
    "        and \"Perceptions of corruption\".\n",
    "        \n",
    "        The data are saved as the following CSV files which are also returned by the\n",
    "        function in the following order:\n",
    "        X_train.csv, y_train.csv, X_test.csv, y_test.csv\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        whdata_excel_file_path\n",
    "            The path to the World Happiness data in Excel format\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train_path\n",
    "            Path to the training data feature matrix\n",
    "        y_train_path\n",
    "            Path to the training data label vector\n",
    "        X_test_path\n",
    "            Path to the testing data feature matrix\n",
    "        y_test_path\n",
    "            Path to the testing data label vector\n",
    "    \"\"\"\n",
    "    # Extract parent directory\n",
    "    data_dir = ...\n",
    "    # Read data\n",
    "    wh_all_df = ...\n",
    "    # Keep only columns of interest (year, Life Ladder, feature cols)\n",
    "    label_col = [\"Life Ladder\"]\n",
    "    feature_cols = [\"Log GDP per capita\", \"Social support\", \"Healthy life expectancy at birth\",\n",
    "                    \"Freedom to make life choices\", \"Generosity\", \"Perceptions of corruption\"]\n",
    "    cols_of_interest = [\"year\"] + label_col + feature_cols\n",
    "    wh_withna_df = ...\n",
    "    # Remove missing values\n",
    "    wh_df = ...\n",
    "    # Create row conditions\n",
    "    rows_2006_to_2018 = ...\n",
    "    rows_2019 = ...\n",
    "    # Create training and testing dataframes\n",
    "    X_train_df = ...\n",
    "    y_train_df = ...\n",
    "    X_test_df = ...\n",
    "    y_test_df = ...\n",
    "    # Assign each file full path to a variable\n",
    "    X_train_path = ...\n",
    "    y_train_path = ...\n",
    "    X_test_path = ... \n",
    "    y_test_path = ...\n",
    "    # Write data to CSV files\n",
    "    X_train_df.to_csv(...)\n",
    "    y_train_df.to_csv(...)\n",
    "    X_test_df.to_csv(...)\n",
    "    y_test_df.to_csv(...)\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you didn't finish\n",
    "Just enter the following commands:\n",
    "```\n",
    "cd\n",
    "cp -R happiness_solutions/part3/* happiness_project/\n",
    "```\n",
    "You are now up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 A: Implement software version control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our first code working, we will track all its future development with git. We will also add a `README` and `LICENSE` file.\n",
    "\n",
    "Importantly, we add a `setup.py` file for anyone downloading our code to easily install it as a package in their python environment by running `python setup.py install`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. (VS Code) Create a markdown `README.md` file describing the project inside the `happiness_project` dir\n",
    "2. (VS Code) Create a `LICENSE` file (e.g. [MIT](https://opensource.org/licenses/MIT), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html)) inside the `happiness_project` dir\n",
    "3. (VS Code) Create a `setup.py` file (cf example below) inside the `happiness_project` dir\n",
    "4. (Terminal 2) Go inside `happiness_project` and init tracking with `git`\n",
    "5. (Browser) Go to Github and create a repository called `happiness_project` (do not initialize with a `README` or `LICENSE` as we already created them)\n",
    "6. (Terminal 2) Follow the instructions on Github to locally add a remote to your Github repository\n",
    "7. (Terminal 2) Push your local repo to Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Git:\n",
    "    * `git init`: initialize the local directory for tracking by git\n",
    "    * `git add .` : stage all changes\n",
    "    * `git commit -m <MESSAGE>`: commit the changes with the message `<MESSAGE>`  \n",
    "    * `git remote add origin <URL>`: set the remote `<URL`> to alias `origin`\n",
    "    * `git branch -M main`: move the current branch to `main` (unchanged if already named `main`)\n",
    "    * `git push -u origin main`: push the changes to the branch main and track the remote branch locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "with open(\"README.md\", \"r\") as readme_file:\n",
    "    long_description = readme_file.read()\n",
    "\n",
    "setup(\n",
    "    name = \"happiness_project\",\n",
    "    version = \"0.0.1\",\n",
    "    author = \"Michael Dayan\",\n",
    "    author_email = \"michael.dayan@fcbg.ch\",\n",
    "    description = \"Predict country happiness from World Happiness Report data\",\n",
    "    long_description = long_description,\n",
    "    long_description_content_type = \"text/markdown\",\n",
    "    url = \"https://github.com/NIDS2020-instructor/happiness_project\",\n",
    "    packages = find_packages(),\n",
    "    classifiers = [\n",
    "        \"Programming Language :: Python :: 3\",\n",
    "        \"License :: OSI Approved :: MIT License\",\n",
    "        \"Operating System :: OS Independent\",\n",
    "        \"Intended Audience :: Science/Research\",\n",
    "        \"Topic :: Scientific/Engineering\"\n",
    "    ],\n",
    "    python_requires='>=3.7',\n",
    "    install_requires=[\"pandas\", \"xlrd\", \"scikit-learn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you didn't finish\n",
    "Just enter the following commands:\n",
    "```\n",
    "cd\n",
    "cp -R happiness_solutions/part4a/* happiness_project/\n",
    "```\n",
    "You are now up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 B: Implement continuous integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important for our code to be continuously tested. This can be done by testing it at each push towards our remote repository before it is merged with our remote master branch. This is a form of continuous integration. We will see how to do this with `Travis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. (VS Code) Add the test functions below to `happyio.py` to test if the CSV files generated all have a non trivial number of lines (e.g. more than 3 entries)\n",
    "2. (Terminal 2) From the `happiness_project` directory, run `pytest happypred/happyio.py` and check they pass\n",
    "3. (Browser) Go to the [Travis website](https://travis-ci.com/signin) and link your Github account so that to associate at least your current NIDS lecture repo\n",
    "4. (VS Code) Copy the content of the Travis YAML file to a new file `.travis.yaml` to save in `happiness_project`\n",
    "5. (Terminal 2) Stage the changes, commit, and push to the remote. Check that the Travis build passes successfully\n",
    "6. (Browser) On the Travis page, click on the \"Build passing\" badge and choose \"Markdown\" format. By copy-pasting this text at the top of your `README.md`, the build status will be seen easily on your Github repo project page.\n",
    "7. OPTIONAL: redo steps 4 and 5 with conda version of Travis yaml file if not used the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Git:\n",
    "    * `git add .` : stage all changes\n",
    "    * `git commit -m <MESSAGE>`: commit the changes with the message `<MESSAGE>`  \n",
    "    * `git push origin main`: push the changes to the branch main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests of whdata_to_csv function\n",
    "\n",
    "def test_whdata_to_csv_Xtrain():\n",
    "    wh_data_path = 'WHR20_DataForTable2.1.xls'\n",
    "    X_train_path, _, _, _ = whdata_to_csv(wh_data_path)\n",
    "    X_train_df = pd.read_csv(X_train_path)\n",
    "    assert(X_train_df.shape[0] > 3)\n",
    "\n",
    "def test_whdata_to_csv_ytrain():\n",
    "    wh_data_path = 'WHR20_DataForTable2.1.xls'\n",
    "    _, y_train_path, _, _ = whdata_to_csv(wh_data_path)\n",
    "    y_train_df = pd.read_csv(y_train_path)\n",
    "    assert(y_train_df.shape[0] > 3)\n",
    "\n",
    "def test_whdata_to_csv_Xtest():\n",
    "    wh_data_path = 'WHR20_DataForTable2.1.xls'\n",
    "    _, _, X_test_path, _ = whdata_to_csv(wh_data_path)\n",
    "    X_test_df = pd.read_csv(X_test_path)\n",
    "    assert(X_test_df.shape[0] > 3)\n",
    "\n",
    "def test_whdata_to_csv_ytest():\n",
    "    wh_data_path = 'WHR20_DataForTable2.1.xls'\n",
    "    _, _, _, y_test_path = whdata_to_csv(wh_data_path)\n",
    "    y_test_df = pd.read_csv(y_test_path)\n",
    "    assert(y_test_df.shape[0] > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple version (to save as .travis.yaml)\n",
    "language: python\n",
    "\n",
    "python:\n",
    "  - \"3.7\"\n",
    "  - \"3.8\"\n",
    "\n",
    "install:\n",
    "  - pip install pytest\n",
    "  - pip install pandas xlrd scikit-learn\n",
    "  - python setup.py install\n",
    "\n",
    "script:\n",
    "  - pytest happypred/happyio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conda version (to save as .travis.yaml)\n",
    "language: python\n",
    "\n",
    "python:\n",
    "  - \"3.7\"\n",
    "  - \"3.8\"\n",
    "\n",
    "install:\n",
    "  # We do this conditionally because it saves us some downloading if the\n",
    "  # version is the same.\n",
    "  - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\n",
    "  - bash miniconda.sh -b -p $HOME/miniconda\n",
    "  - source \"$HOME/miniconda/etc/profile.d/conda.sh\"\n",
    "  - hash -r\n",
    "  - conda config --set always_yes yes --set changeps1 no\n",
    "  - conda update -q conda\n",
    "  # Useful for debugging any issues with conda\n",
    "  - conda info -a\n",
    "\n",
    "  - conda create -q -n happy_test python=$TRAVIS_PYTHON_VERSION pytest pandas xlrd scikit-learn\n",
    "  - conda activate happy_test\n",
    "  - python setup.py install\n",
    "\n",
    "script:\n",
    "  - pytest happypred/happyio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you didn't finish\n",
    "Just enter the following commands:\n",
    "```\n",
    "cd\n",
    "cp -R happiness_solutions/part4b/* happiness_project/\n",
    "```\n",
    "You are now up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using sklearn to predict a country happiness score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a model on training data, and then save it for it to be easily reused for predictions by users of our python package.\n",
    "\n",
    "For maximum simplification we will just use a linear regression model, but this should be replaced with something else. For example you could try using a pipeline including both `sklearn.preprocessing.StandardScaler` and `sklearn.linear_model.Ridge` (linear regression with L2 regularization) by using 5-fold CV for the outer loop, and 5-fold CV for the inner loop to tune the `alpha` hyper-parameter. The tuning could be done with GridSearch as we did during the ML lecture, or even more easily with the convenient `sklearn.linear_model.RidgeCV` estimator which as built-in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "The first 4 Jupyter Explore steps are optional if you feel like coding in VS Code directly.\n",
    "1. (Jupyter Explore) Create a `LinearRegression()` instance and assign it to an `lm` variable\n",
    "2. (Jupyter Explore) Read `train.csv` and `test.csv` in the `X_train_df` and `y_train_df` variables respectively\n",
    "3. (Jupyter Explore) Fit the `lm` model to `X_train_df` and `y_train_df`\n",
    "4. (Jupyter Explore) Save the model with the pickle module (cf help below) in the file `lm_model.pkl`\n",
    "5. (VS Code) Write a function `hwtrain` in a new `happyml.py` module file, which:\n",
    "    * has two CSV files `X_train_csv` and `y_train_csv` has inputs\n",
    "    * has a `model` as input, which default (and for now only) value is `lm` \n",
    "    * fits a linear regression model if `model` is `lm`\n",
    "    * pickles the model and returns the path to the pickled file\n",
    "6. (VS Code) Write a function `hwpredict` in the `happyml.py` module file you created, which:\n",
    "    * has the name of a model as input (default should be `lm`)\n",
    "    * has single-value features has inputs (`log_gdp`, `social`, `life_exp`, `freedom`, `generosity`, `corruption`) \n",
    "    * reads the pickled model to predict the output\n",
    "    * returns the predicted happiness score\n",
    "7. (VS Code) Update the package `__init__` function to make available for import the functions `hwtrain` and `hwpredict`\n",
    "8. (Jupyter Explore) Test that the imported functions work correctly\n",
    "9. (VS Code) Add a test to `happyml.py` to make sure the predicted happiness score is between 0 and 10, and update the travis file accordingly (for `pytest`)\n",
    "10. (Terminal 2) Stage and commit the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* Python package `sklearn`:\n",
    "    * `<model>.fit(X, y)` to fit the estimator `<model>` from the feature matrix `X` and the labels `y`\n",
    "    * `y_pred = <model>.predict(X)` to predict the label `y_pred` from a given feature matrix `X`    \n",
    "* Git\n",
    "    * `git add .` : stage all changes\n",
    "    * `git commit -m <MESSAGE>`: commit the changes with the message `<MESSAGE>`  \n",
    "    * `git push origin main`: push the changes to the branch main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-77-8471e172250c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-8471e172250c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    * ```with open(<filename>, 'wb') as file:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# To save the fitted model <model> in file <filename>\n",
    "with open(<filename>, 'wb') as file:\n",
    "    pickle.dump(<model>, file) \n",
    "# To load the fitted model <model> from file <filename>\n",
    "with open(<filename>, 'rb') as file:\n",
    "    <model> = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you didn't finish\n",
    "Just enter the following commands:\n",
    "```\n",
    "cd\n",
    "cp -R happiness_solutions/part5/* happiness_project/\n",
    "```\n",
    "You are now up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Package demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new Jupyter notebook, not to explore and prototype code as you did so far, but to illustrate how to use your final package. Use the `X_test.csv` and `y_test.csv` file to show how to use your package to make predictions and to demonstrate the performance of your model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:shared-ds38]",
   "language": "python",
   "name": "conda-env-shared-ds38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
